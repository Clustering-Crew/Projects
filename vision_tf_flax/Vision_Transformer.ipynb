{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMaybzjqmSbq"
      },
      "source": [
        "# Vision Transformer (PyTorch Lightning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL1uYbjY6t0e"
      },
      "source": [
        "## Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y3BAPq7pEIGe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "try:\n",
        "    import flax\n",
        "except ImportError:\n",
        "    %pip install flax\n",
        "    import flax\n",
        "from flax import nnx\n",
        "try:\n",
        "    import jax\n",
        "except ImportError:\n",
        "    %pip install jax\n",
        "    import jax\n",
        "import jax.numpy as jnp\n",
        "try:\n",
        "    import optax\n",
        "except ImportError:\n",
        "    %pip install optax\n",
        "    import optax\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wuGiP_XXhuE7"
      },
      "outputs": [],
      "source": [
        "# Embedding Class\n",
        "class Embed(nnx.Module):\n",
        "\n",
        "    # Embedding = Patch Embedding + Cls Token + Pos Embedding\n",
        "    def __init__(self, config, rngs):\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.image_height = config[\"image_size\"]\n",
        "        self.image_width = config[\"image_size\"]\n",
        "        self.batch_size = config[\"batch\"]\n",
        "        self.patch_count = (self.image_height * self.image_width) // self.patch_size ** 2\n",
        "        self.rng = rngs\n",
        "\n",
        "        self.proj_layer = nnx.Conv(\n",
        "            in_features=3,\n",
        "            out_features=self.embed_dim,\n",
        "            kernel_size=(self.patch_size, self.patch_size),\n",
        "            strides=self.patch_size,\n",
        "            rngs=self.rng\n",
        "        )\n",
        "\n",
        "        self.cls_token = nnx.Param(\n",
        "            jax.random.normal(self.rng.params(), (1, 1, self.embed_dim))\n",
        "        )\n",
        "        \n",
        "        self.pos_embedding = nnx.Param(\n",
        "            jax.random.normal(self.rng.params(), (self.batch_size, self.patch_count + 1, self.embed_dim))\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.proj_layer(x)\n",
        "        x = jnp.reshape(x, (x.shape[0], (x.shape[1] * x.shape[2]), x.shape[3]))\n",
        "        self.cls_tokens = jnp.tile(self.cls_token, [self.batch_size, 1, 1])\n",
        "        x = jnp.concatenate([x, self.cls_tokens], axis=1)\n",
        "\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "# Attention Head\n",
        "class AttentionHead(nnx.Module):\n",
        "    def __init__(self, embed_dim, attention_head_size, bias=True):\n",
        "        self.embed_dim= embed_dim\n",
        "        self.attention_head_size = attention_head_size\n",
        "\n",
        "        # Query, Key and Value Weight matrices\n",
        "        self.q_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.k_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.v_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q_x, k_x, v_x = self.q_w(x), self.k_w(x), self.v_w(x)\n",
        "\n",
        "        # Calculate QK^T/sqrt(dk)\n",
        "        attn_out = jnp.matmul(q_x, jnp.matrix_transpose(k_x)) / jnp.sqrt(self.attention_head_size)\n",
        "        # Apply softmax\n",
        "        softmax_out = nnx.softmax(attn_out)\n",
        "        # Obtain the attention value with value\n",
        "        attn_value = jnp.matmul(softmax_out, v_x)\n",
        "\n",
        "        return attn_value\n",
        "\n",
        "# Multiheadattention\n",
        "class MultiHeadAttention(nnx.Module):\n",
        "    def __init__(self, config):\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.num_of_heads = config[\"num_of_heads\"]\n",
        "\n",
        "        self.attn_head_size = self.embed_dim // self.num_of_heads\n",
        "        self.all_head_size = self.attn_head_size * self.num_of_heads\n",
        "\n",
        "        self.heads = []\n",
        "\n",
        "        for _ in range(self.num_of_heads):\n",
        "            self.attn_head = AttentionHead(\n",
        "                embed_dim=self.embed_dim,\n",
        "                attention_head_size=self.attn_head_size\n",
        "            )\n",
        "            self.heads.append(self.attn_head)\n",
        "\n",
        "        self.linear_proj = nnx.Linear(in_features=self.all_head_size, out_features=self.embed_dim, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.dropout = nnx.Dropout(0.3, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        attn_outputs = [head(x) for head in self.heads]\n",
        "\n",
        "        concat_output = jnp.concatenate(attn_outputs, axis=-1)\n",
        "        proj_output = self.linear_proj(concat_output)\n",
        "        proj_output = self.dropout(x)\n",
        "\n",
        "        return proj_output\n",
        "\n",
        "# MLP class\n",
        "class MLP(nnx.Module):\n",
        "    def __init__(self, config):\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.intermediate_size = config[\"intermediate_size\"]\n",
        "\n",
        "        self.linear1 = nnx.Linear(in_features=self.embed_dim, out_features=self.intermediate_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.linear2= nnx.Linear(in_features=self.intermediate_size, out_features=self.embed_dim, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.dropout = nnx.Dropout(0.3, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = nnx.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Single block\n",
        "class Block(nnx.Module):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.norm = nnx.BatchNorm(num_features=self.embed_dim, rngs=nnx.Rngs(0))\n",
        "        self.mha = MultiHeadAttention(self.config)\n",
        "        self.mlp = MLP(self.config)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        norm_out = self.norm(x)\n",
        "        attn_out = self.mha(x)\n",
        "        attn_out = x + attn_out\n",
        "        norm_out = self.norm(attn_out)\n",
        "        mlp_out = self.mlp(norm_out)\n",
        "        block_out = mlp_out + norm_out\n",
        "\n",
        "        return block_out\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nnx.Module):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.num_of_blocks = config[\"num_of_blocks\"]\n",
        "\n",
        "        self.blocks = []\n",
        "\n",
        "        for _ in range(self.num_of_blocks):\n",
        "            block = Block(self.config)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "\n",
        "    def __call__(self, x):\n",
        "        all_attns = []\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "# ViT Classifier\n",
        "class ViT(nnx.Module):\n",
        "    def __init__(self, config):\n",
        "        self.embed_layer = Embed(config, nnx.Rngs(0))\n",
        "        self.encoder = Encoder(config)\n",
        "        self.classifier = nnx.Linear(in_features=config[\"embed_dim\"], out_features=config[\"no_of_classes\"], use_bias=True, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.embed_layer(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.classifier(x[:, 0, :])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading dataset and Creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# Load the NPY files of CIFAR10\n",
        "base_path = r\"D:\\cifar-10-python\\cifar-10-batches-py\"\n",
        "\n",
        "train_val_images = jnp.load(os.path.join(base_path, \"train_val_data.npy\"))\n",
        "train_val_labels = jnp.load(os.path.join(base_path, \"train_val_labels.npy\"))\n",
        "\n",
        "train_val_images = train_val_images / 255.0\n",
        "\n",
        "print(jnp.unique_counts(train_val_labels)[1].shape[0])\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_val_images, train_val_labels, test_size=0.8, random_state=42)\n",
        "print(x_train.shape)\n",
        "\n",
        "class Dataloader:\n",
        "    def __init__(self, x, y, batch, shuffle):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.batch = batch\n",
        "        self.shuffle = shuffle\n",
        "        self.n_samples = self.x.shape[0]\n",
        "        self.total_batches = self.n_samples // self.batch\n",
        "        self.indices = jnp.arange(self.n_samples)\n",
        "        self.key = jax.random.PRNGKey(0)\n",
        "        if self.shuffle:\n",
        "            jax.random.permutation(self.key, self.indices, axis=0)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.total_batches\n",
        "    \n",
        "    def __iter__(self):\n",
        "        self.current_batch = 0\n",
        "        if self.shuffle:\n",
        "            jax.random.permutation(self.key, self.indices, axis=0)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.current_batch >= self.total_batches:\n",
        "            raise StopIteration\n",
        "        \n",
        "        start_idx = self.current_batch * self.batch\n",
        "        end_idx = min(start_idx + self.batch, self.n_samples)\n",
        "\n",
        "        x_batch = self.x[start_idx:end_idx, :, :, :]\n",
        "        y_batch = self.y[start_idx:end_idx]\n",
        "\n",
        "        self.current_batch += 1\n",
        "\n",
        "        return x_batch, y_batch\n",
        "    \n",
        "    def get_batch(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_batch = 0\n",
        "        if self.shuffle:\n",
        "            jax.random.permutation(self.key, self.indices, axis=0)\n",
        "\n",
        "\n",
        "train_dataloader = Dataloader(\n",
        "    x_train, y_train, batch=16, shuffle=False\n",
        ")\n",
        "\n",
        "val_dataloader = Dataloader(\n",
        "    x_val, y_val, batch=16, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "def loss_fn(model, images, labels):\n",
        "    logits = model(images)\n",
        "    loss = optax.losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "    return loss, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Training and validation step\n",
        "def train_step(model: nnx.Module, optim: nnx.Optimizer, images: jax.Array, labels: jax.Array):\n",
        "\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, images, labels)\n",
        "    \n",
        "    optim.update(model, grads)\n",
        "    return loss\n",
        "\n",
        "def eval_step(model: nnx.Module, images: jax.Array, labels: jax.Array, eval_metrics: nnx.MultiMetric):\n",
        "    \n",
        "    loss, logits = loss_fn(model, images, labels)\n",
        "    eval_metrics.update(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        labels=labels\n",
        "    )\n",
        "\n",
        "train_step = nnx.jit(train_step)\n",
        "eval_step = nnx.jit(eval_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_metrics = nnx.MultiMetric(\n",
        "    loss = nnx.metrics.Average(\"loss\"),\n",
        "    accuracy = nnx.metrics.Accuracy(),\n",
        ")\n",
        "\n",
        "train_history = {\n",
        "    \"train_loss\": [], \n",
        "}\n",
        "\n",
        "val_history = {\n",
        "    \"val_loss\": [],\n",
        "    \"val_accuracy\": []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure optimizer, loss, metrics\n",
        "\n",
        "tqdm_bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
        "\n",
        "lr_0 = 0.001\n",
        "lr_f = 1e-5\n",
        "momemtum = 0.8\n",
        "epochs = 100\n",
        "total_steps = x_train.shape[0] // 16\n",
        "\n",
        "config_dict = {\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 128,\n",
        "    \"image_size\": 32,\n",
        "    \"batch\": 16,\n",
        "    \"no_of_classes\": 10,\n",
        "    \"num_of_blocks\": 6,\n",
        "    \"num_of_heads\": 12,\n",
        "    \"intermediate_size\": 4 * 128,\n",
        "}\n",
        "\n",
        "\n",
        "model = ViT(config_dict)\n",
        "\n",
        "lr_schedule = optax.linear_schedule(lr_0, lr_f, epochs * total_steps)\n",
        "\n",
        "optim = nnx.Optimizer(\n",
        "    model, optax.adam(lr_schedule), wrt=nnx.Param\n",
        ")\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "\n",
        "    with tqdm.tqdm(\n",
        "        desc=f\"[Train] epoch: {epoch} / {epochs}, \",\n",
        "        total=total_steps,\n",
        "        bar_format=tqdm_bar_format,\n",
        "        leave=True\n",
        "    ) as pbar:\n",
        "        for images, labels in train_dataloader:\n",
        "            loss = train_step(model, optim, images, labels)\n",
        "            train_history[\"train_loss\"].append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "            pbar.update(1)\n",
        "    \n",
        "def eval_model(epoch):\n",
        "    model.eval()\n",
        "    eval_metrics.reset()\n",
        "\n",
        "    for val_images, val_labels in val_dataloader:\n",
        "        loss = eval_step(model, val_images, val_labels, eval_metrics)\n",
        "    \n",
        "    for metric, value in eval_metrics.compute().items():\n",
        "        val_history[f\"val_{metric}\"].append(value)\n",
        "\n",
        "    print(f\"[Val] epoch: {epoch + 1} / {epochs}\")\n",
        "    print(f\"Loss: {val_history['val_loss'][-1]:0.4f}\")\n",
        "    print(f\"Accuracy: {val_history['val_accuracy'][-1]:0.4f}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 0 / 100, [0/625] [00:00<?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 0 / 100, [625/625], loss=2.15 [02:25<00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 1 / 100\n",
            "Loss: 2.1863\n",
            "Accuracy: 0.1558\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 1 / 100, [625/625], loss=2.03 [01:48<00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 2 / 100\n",
            "Loss: 2.1759\n",
            "Accuracy: 0.1653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 2 / 100, [625/625], loss=2.11 [01:49<00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 3 / 100\n",
            "Loss: 2.1741\n",
            "Accuracy: 0.1599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 3 / 100, [625/625], loss=2.09 [01:51<00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 4 / 100\n",
            "Loss: 2.1599\n",
            "Accuracy: 0.1690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 4 / 100, [625/625], loss=2.07 [01:52<00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 5 / 100\n",
            "Loss: 2.1661\n",
            "Accuracy: 0.1788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 5 / 100, [625/625], loss=2.15 [01:49<00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 6 / 100\n",
            "Loss: 2.1614\n",
            "Accuracy: 0.1746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 6 / 100, [625/625], loss=2.1 [01:48<00:00] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 7 / 100\n",
            "Loss: 2.1549\n",
            "Accuracy: 0.1822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 7 / 100, [307/625], loss=2.1 [00:45<00:39] Exception ignored in: <function _xla_gc_callback at 0x00000270BBA87380>\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\jax\\_src\\lib\\__init__.py\", line 112, in _xla_gc_callback\n",
            "KeyboardInterrupt: \n",
            "[Train] epoch: 7 / 100, [625/625], loss=2.07 [01:37<00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Val] epoch: 8 / 100\n",
            "Loss: 2.1694\n",
            "Accuracy: 0.1649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 8 / 100, [121/625], loss=2.23 [00:20<01:22]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 8 / 100, [168/625], loss=2.3 [00:28<01:07] "
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    train_one_epoch(epoch)\n",
        "    eval_model(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
